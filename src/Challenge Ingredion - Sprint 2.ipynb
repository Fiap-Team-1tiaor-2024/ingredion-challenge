{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goJQLbQYgcVE"
      },
      "source": [
        "\n",
        "\n",
        "# Challenge Ingredion - Sprint 2\n",
        "Este notebook foi desenvolvido para o Challenge em parceria com a Ingredion, na Sprint 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUA6WXnJe1w"
      },
      "source": [
        "Conectando o caminho do dataset diretamente do Drive.\n",
        "\n",
        "*(Há a opção de pegarmos as bases de dados e subir nas pastas do notebook, assim rodando localmente.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X0a8KauzgCU9"
      },
      "outputs": [],
      "source": [
        "dataset = \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/dado-estruturados.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMri9CY9yw2L"
      },
      "source": [
        "Definindo o código para conexão com o Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VsDaLXvqiDSj",
        "outputId": "f21075e6-b4b8-4d64-f42d-0a52b5d3b24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tn0JQ2Gy7HC"
      },
      "source": [
        "Importantos as libs necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y1bxQPFJyu0Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pgbfOdPZT5k"
      },
      "source": [
        "### Análise exploratória dos dados\n",
        "\n",
        "Aqui damos início a nossa análise exploratória dos dados para identificarmos nossas variáveis chaves do projeto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPGBUGY-Z07D"
      },
      "outputs": [],
      "source": [
        "df_structured = pd.read_csv(dataset)\n",
        "\n",
        "df_structured.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK8sTQtyIXdn"
      },
      "source": [
        "Definindo a variável do dataframe e **exibindo as 5 primeiras linhas** e **as últimas 5 linhas** do df.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PzNhrUqiYH4v"
      },
      "outputs": [],
      "source": [
        "df_structured.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zYjQ70bKbPCH"
      },
      "outputs": [],
      "source": [
        "df_structured.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42cNIokVbxO2"
      },
      "source": [
        "Obtém **informações gerais** de cada coluna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V_hN7CxxYJ3S"
      },
      "outputs": [],
      "source": [
        "df_structured.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZx8qBzHbpbW"
      },
      "source": [
        "Verifica colunas que possui **valores nulos**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LSVsNOd1aZbl"
      },
      "outputs": [],
      "source": [
        "df_structured.notnull()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLl_320deaor"
      },
      "source": [
        "Conta **se há colunas nulas**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jL_IgNTMdM_v"
      },
      "outputs": [],
      "source": [
        "df_structured.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOCFvPFmfbWb"
      },
      "source": [
        "Conta as colunas **duplicadas**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXQU0BsLeZZB"
      },
      "outputs": [],
      "source": [
        "df_structured.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbVe3b03kWKU"
      },
      "source": [
        "\n",
        "### Identificando Outliers\n",
        "\n",
        "Após uma breve análise exploratória, vamos tentar identificar os outliers do nosso dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfyh7E4hvw91"
      },
      "source": [
        "Utilizando o gráfico **Boxplot** para identificar outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V6dYKBHxsZLP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df_structured.select_dtypes(include=['number']))\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Boxplot das variáveis para identificação de outliers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHXqAAxzyN8O"
      },
      "source": [
        "Podemos ver que a **Quantidade Produzida (Toneladas)** aprensenta um número maior de outliers. Outras variáveis também apresentam, mas são outliers quase irrelevantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjlIeuoqvR_f"
      },
      "source": [
        "Usando a **Matriz de Correlação Visual** para entender os padrões, utilizando Seaborn.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gkr0fILfHI9"
      },
      "outputs": [],
      "source": [
        "df_apenas_numericos = df_structured.select_dtypes(include=['number'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_apenas_numericos.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZbCYTqyh8j"
      },
      "source": [
        "**Forte correlação entre:**\n",
        "\n",
        "* Ano e valor da produção **(0.89)**.\n",
        "* Área plantada ou destinada à colheita (hectares) e Percentual Geral **(0.89)**.\n",
        "* Área colhida e área plantada ou destinada à colheita **(0.75)**.\n",
        "* Quantidade produzida (Toneladas) e Rendimento médio da produção **(0.92)**.\n",
        "\n",
        "**Correlação Fraca com o Rendimento:**\n",
        "\n",
        "* Área plantada ou destinada à colheita (hectares) e Valor da produção (-0.41).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3W-APKLu017"
      },
      "source": [
        "Utilizando a regra do **IQR (Intervalo Interquatil)** para remover outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2drz5SMatLpd"
      },
      "outputs": [],
      "source": [
        "variaveis_importantes = [\"Rendimento médio da produção (Quilogramas por Hectare)\",\n",
        "                         \"Quantidade produzida (Toneladas)\",\n",
        "                         \"Área colhida (Hectares)\",\n",
        "                         \"Valor da produção (Mil Reais)\",\n",
        "                        ]\n",
        "\n",
        "df_filtrado = df_structured.copy()\n",
        "\n",
        "for coluna in variaveis_importantes:\n",
        "    Q1 = df_filtrado[coluna].quantile(0.25)\n",
        "    Q3 = df_filtrado[coluna].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    limite_inferior = Q1 - 1.5 * IQR\n",
        "    limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "    df_filtrado = df_filtrado[df_filtrado[coluna].between(limite_inferior, limite_superior)]\n",
        "\n",
        "df_filtrado.shape\n",
        "df_filtrado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFIVW1xHN8sg"
      },
      "source": [
        "Agora posssuímos um Dataframe **filtrado** e informações mais concretas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zLnZPiHDF5"
      },
      "source": [
        "### Aplicação da Técnica de Segmentação - SAM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cu1B8wmzld8p"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision albumentations opencv-python matplotlib numpy rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4lo-9mHld8p"
      },
      "outputs": [],
      "source": [
        "# Criar symlink para o dataset\n",
        "!ln -s '/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/not-structured'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6XwTj74ld8p"
      },
      "outputs": [],
      "source": [
        "# Celda 3: Classe Dataset\n",
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SatelliteDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "        # Verifique se todos os arquivos têm máscaras correspondentes\n",
        "        for img_name in self.images:\n",
        "            mask_name = img_name.replace(\".jpg\", \"_mask.png\")\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "            if not os.path.exists(mask_path):\n",
        "                raise FileNotFoundError(f\"Máscara não encontrada: {mask_path}\")\n",
        "\n",
        "    def __len__(self): # Added __len__ method\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        mask_name = img_name.replace(\".jpg\", \"_mask.png\")\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Leitura com verificação explícita\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Imagem não pode ser lida: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            raise ValueError(f\"Máscara não pode ser lida: {mask_path}\")\n",
        "\n",
        "        mask = mask / 255.0  # Divisão por float\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Transformações\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Celda 4: Criação dos DataLoaders\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "train_dataset = SatelliteDataset(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/train/images\",\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/train/masks\",\n",
        "    train_transform\n",
        ")\n",
        "\n",
        "val_dataset = SatelliteDataset(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/val/images\",\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/val/masks\",\n",
        "    val_transform\n",
        ")\n",
        "\n",
        "test_dataset = SatelliteDataset(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/test/images\",\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/test/masks\",\n",
        "    val_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx0gBlqkld8p"
      },
      "outputs": [],
      "source": [
        "# Celda 5: Modelo DeepLabV3+\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4] = torch.nn.Conv2d(256, 1, kernel_size=1)  # 1 classe\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynjnWFqld8p"
      },
      "outputs": [],
      "source": [
        "# Celda 6: Função de cálculo de IoU (ajustada para NumPy)\n",
        "def calculate_iou(pred, target):\n",
        "    # Verifica se os inputs são tensores do PyTorch\n",
        "    is_tensor = isinstance(pred, torch.Tensor)\n",
        "\n",
        "    # Converte para booleanos\n",
        "    if is_tensor:\n",
        "        pred = pred.bool()       # Método do PyTorch\n",
        "        target = target.bool()   # Método do PyTorch\n",
        "        intersection = torch.logical_and(pred, target).sum()\n",
        "        union = torch.logical_or(pred, target).sum()\n",
        "    else:\n",
        "        pred = pred.astype(bool)    # Método do NumPy\n",
        "        target = target.astype(bool)  # Método do NumPy\n",
        "        intersection = np.logical_and(pred, target).sum()\n",
        "        union = np.logical_or(pred, target).sum()\n",
        "\n",
        "    # Calcula IoU e converte para float (evita erro de tipo)\n",
        "    iou = (intersection / union) if union != 0 else 0.0\n",
        "    return iou.item() if is_tensor else iou  # .item() para tensores\n",
        "\n",
        "# Celda 7: Loop de Treinamento\n",
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Treino\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, masks in train_loader:\n",
        "        images = images.permute(0, 3, 1, 2).float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        if images.shape[0] == 1:  # Check if batch size is 1\n",
        "            # Pad the batch to increase batch size\n",
        "            images = torch.cat([images] * 2, dim=0) # Duplicate the image to create a batch of 2\n",
        "            masks = torch.cat([masks] * 2, dim=0) # Duplicate the mask to create a batch of 2\n",
        "\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs.squeeze(1), masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validação\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_iou = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.permute(0, 3, 1, 2).float().to(device)\n",
        "            masks = masks.float().to(device)\n",
        "\n",
        "            outputs = model(images)['out']\n",
        "            val_loss += criterion(outputs.squeeze(1), masks).item()\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            val_iou += calculate_iou(preds.cpu(), masks.cpu())\n",
        "\n",
        "    # Salvar melhor modelo\n",
        "    avg_val_iou = val_iou / len(val_loader)\n",
        "    if avg_val_iou > best_iou:\n",
        "        best_iou = avg_val_iou\n",
        "        torch.save(model.state_dict(), '/content/best_model.pth')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Val IoU: {avg_val_iou:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpwKiz3Bld8p"
      },
      "outputs": [],
      "source": [
        "# Celda 8: Carregar melhor modelo\n",
        "model.load_state_dict(torch.load('/content/best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Celda 9: Teste\n",
        "test_iou = 0.0\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images = images.permute(0, 3, 1, 2).float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        outputs = model(images)['out']\n",
        "        test_loss += criterion(outputs.squeeze(1), masks).item()\n",
        "\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        test_iou += calculate_iou(preds.cpu(), masks.cpu())\n",
        "\n",
        "print(f'\\nTest Loss: {test_loss/len(test_loader):.4f} | Test IoU: {test_iou/len(test_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjX0_YVBld8q"
      },
      "outputs": [],
      "source": [
        "# Celda 10: Visualização\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "idx = 0\n",
        "image, mask = test_dataset[idx]  # A imagem original está em (H, W, C)\n",
        "\n",
        "# Aplicar transformações na imagem no formato (H, W, C)\n",
        "transformed = val_transform(image=image, mask=mask)\n",
        "transformed_image = transformed['image']  # Agora (H, W, C) após transformações\n",
        "\n",
        "# Converter para tensor e ajustar dimensões\n",
        "input_tensor = torch.from_numpy(transformed_image).permute(2, 0, 1)  # (C, H, W)\n",
        "input_tensor = input_tensor.unsqueeze(0).float().to(device)  # Adicionar dimensão de batch\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)['out'].squeeze()\n",
        "pred_mask = (torch.sigmoid(output) > 0.5).cpu().numpy()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Imagem Original (desnormalize se necessário)\n",
        "plt.subplot(1,3,1)\n",
        "original_image = image.numpy() if isinstance(image, torch.Tensor) else image  # Garante que é um array NumPy\n",
        "plt.imshow(original_image)\n",
        "plt.title('Imagem Original')\n",
        "\n",
        "# Máscara Real\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title('Máscara Real')\n",
        "\n",
        "# Predição\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(pred_mask, cmap='gray')\n",
        "plt.title(f'Predição (IoU: {calculate_iou(pred_mask, mask):.2f})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbrZDLBZld8q"
      },
      "outputs": [],
      "source": [
        "# Celda 11: Função para predição\n",
        "def predict_image(image_path):\n",
        "    # Verifica se a imagem existe\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Arquivo não encontrado: {image_path}\")\n",
        "\n",
        "    # Carrega a imagem\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Falha ao ler a imagem: {image_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converte para RGB\n",
        "\n",
        "    # Aplica as transformações e converte para tensor\n",
        "    augmented = val_transform(image=image)\n",
        "    input_tensor = augmented['image']  # Assume-se que é um array NumPy\n",
        "\n",
        "    # Converte para tensor do PyTorch e ajusta as dimensões\n",
        "    input_tensor = torch.from_numpy(input_tensor).float()  # Converte para tensor\n",
        "    input_tensor = input_tensor.permute(2, 0, 1)          # Muda de (H, W, C) para (C, H, W)\n",
        "    input_tensor = input_tensor.unsqueeze(0).to(device)    # Adiciona dimensão do batch\n",
        "\n",
        "    # Predição\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)['out'].squeeze()\n",
        "\n",
        "    return (torch.sigmoid(output) > 0.5).cpu().numpy()\n",
        "\n",
        "# Exemplo de uso\n",
        "prediction = predict_image('/content/drive/MyDrive/Colab Notebooks/FIAP/Fase 6/Atividade - Challenge/data/not-structured/img-satelite-fazenda-yrere.png')\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgt_MBaW73c-"
      },
      "source": [
        "## Desenvolvendo os Modelos Preditivos\n",
        "Agora vamos desenvolver 5 tipos de modelos preditivos para prever o rendimento da safra.\n",
        "\n",
        "Foi usado os seguintes modelos:\n",
        "- Ridge;\n",
        "- Regressão Linear;\n",
        "- Árvore de Regressão;\n",
        "- Floresta de Regressão;\n",
        "- KNN de Regressão;\n",
        "- SVR;\n",
        "- GradientBoostingRegressor;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m3eSs0OeuBl"
      },
      "source": [
        "Padronizando os dados com o **RobustScaler.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMgiI8Hiw40E"
      },
      "outputs": [],
      "source": [
        "X = df_filtrado.drop(['Yield', 'Crop'], axis=1)\n",
        "y = df_filtrado['Yield']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnVIV4YaUd7D"
      },
      "outputs": [],
      "source": [
        "ridge_model = Ridge(alpha=0.01, fit_intercept=True, solver='auto')\n",
        "ridge_model.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "print(\"Ridge Regressão\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_ridge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlLwgBFRBN6x"
      },
      "outputs": [],
      "source": [
        "linear_model = LinearRegression(fit_intercept=True, copy_X=False, n_jobs=-1)\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "print(\"Regressão Linear\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_linear))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_linear))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "asL2lWJIEgBS"
      },
      "outputs": [],
      "source": [
        "arvore_decisao = DecisionTreeRegressor(max_depth=1, min_samples_split=2, min_samples_leaf=10, random_state=42)\n",
        "arvore_decisao.fit(X_train, y_train)\n",
        "y_pred_arvore = arvore_decisao.predict(X_test)\n",
        "\n",
        "print(\"Árvore de Decisão\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_arvore))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_arvore))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WHcjHhMMEoi5"
      },
      "outputs": [],
      "source": [
        "floresta_aleatoria = RandomForestRegressor(n_estimators=100, max_depth=4, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
        "floresta_aleatoria.fit(X_train, y_train)\n",
        "\n",
        "y_pred_floresta = floresta_aleatoria.predict(X_test)\n",
        "print(\"Floresta Aleatória\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_floresta))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_floresta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vgAN5WNOFRk3"
      },
      "outputs": [],
      "source": [
        "knn_model = KNeighborsRegressor(n_neighbors=1, weights='uniform', algorithm='brute', p=1)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "print(\"K-Nearest Neighbors\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_knn))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_knn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Uu6eyfZFcK0"
      },
      "outputs": [],
      "source": [
        "svr_model = SVR(kernel='sigmoid', C=1.0, epsilon=0.4, gamma='scale')\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svr = svr_model.predict(X_test)\n",
        "print(\"Support Vector Regression\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_svr))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_svr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJZ5FzuCFoS2"
      },
      "outputs": [],
      "source": [
        "gb_model = GradientBoostingRegressor(min_samples_split=10, learning_rate=0.03, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "print(\"Gradient Boosting\")\n",
        "print(\"Erro Quadrático Médio:\", mean_squared_error(y_test, y_pred_gb))\n",
        "print(\"R-quadrado:\", r2_score(y_test, y_pred_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKMKXcIROIpv"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Os gráficos mostra **a relação entre os valores reais** e as **previsões de rendimento do modelo de Regressão Linear** e o **modelo Ridge Regression**. Embora a maioria das previsões esteja próxima dos valores reais, há um ponto fora da curva, indicando um possível outlier ou uma limitação do modelo.\n",
        "\n",
        "O uso do RobustScaler **ajudou a minimizar o impacto dos outliers**. No geral, os modelos de Regressão Linear e Ridge Regression apresentaram desempenho estável, mantendo um coeficiente de determinação (R²) **acima de 80%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MgJK0w2gG8cH"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred_linear)\n",
        "plt.xlabel(\"Valores Reais (Rendimento da Safra)\")\n",
        "plt.ylabel(\"Valores Previstos\")\n",
        "plt.title(\"Ridge Regression - Rendimento Real vs. Previsto\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AETwD5gFjDo"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_pred_ridge)\n",
        "plt.xlabel(\"Valores Reais (Rendimento da Safra)\")\n",
        "plt.ylabel(\"Valores Previstos\")\n",
        "plt.title(\"Ridge Regression - Rendimento Real vs. Previsto\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XbVe3b03kWKU",
        "fgt_MBaW73c-",
        "BKMKXcIROIpv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}